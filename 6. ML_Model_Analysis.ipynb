{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/ml/LightGradientBoostingonElasticNetPredictions.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='images/ml/LightGradientBoostingonElasticNetPredictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model analysis\n",
    "\n",
    "Ingest data and initial exploratory data analysis where the data is fed into three preprocessing steps\n",
    "\n",
    "__Numeric Variables:__ Variable that are numbers. (e.g. quantitative variables). Examples include 0, 1, -1, 4.5, and π.\n",
    "\n",
    "__Text Variable:__ Variables that are text (more than a single word). Example: \"The cool robot ran a model.\" or \"おはようございます。\"\n",
    "\n",
    "__Categorical Variables:__ Variables that are categories. (e.g. qualitative or character variables). Examples include \"Female\", \"Massachusetts\", \"синій\", and \"赤\".\n",
    "\n",
    "__Numeric Cleansing:__ Inpute missing/disguised missing values on numeric variables with their median and create indicator variables to mark records with data quality issues\n",
    "\n",
    "__Matrix of word-grams occurrences:__ Convert raw text fields into a document-term matrix. Based on [scikit-learn TfidfVectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
    "\n",
    "__Standardize:__ Standardize features by removing the median and scaling to unit variance or mean absolute deviation. \n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data. For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "__One-Hot:__ (or dummy-variable) transformation of categorical features. This transformer will do a binary one-hot (aka one-of-K) coding. One boolean-valued feature is constructed for each of the possible string values that the feature can take. For inputs with only 2 unique values, only one boolean-valued feature will be constructed. This encoding is needed for feeding categorical data to many estimators, notably linear models and SVMs.\n",
    "\n",
    "__Ordinal encoding of categorical variables:__ Converts categorical variables to an ordinal scale. Ordinal transformation of categorical features. Recodes categorical features as integers based on either the lexicographic ordering of the categorical values, the frequency of the categorical values, the response or randomly. The ordinal scale is 0 to (unique values of categorical_var) - 1. Rare categories (=other) and missing values are encoded as -1 and -2, respectively. The mapping will be done based on either the lexicographic ordering of the categorical values, the frequency of the levels, the response, or randomly. Ordinal encoding works well for tree-based methods; it usually gives equal performance as one-hot encoding but requires much less computational resources (memory and cpu). Ordinal encoding; however, does not work for linear methods.\n",
    "\n",
    "__Elastic-Net Classifier (L2 / Binomial Deviance):__ Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier. \n",
    "\n",
    "__Light Gradient Boosting on ElasticNet Predictions__ Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals. Light Gradient Boosted Machine Classifier with Early Stopping. The Classifier uses the LightGBM implementation of Gradient Boosted Trees. LightGBM is a gradient boosting framework designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "1. Faster training speed and higher efficiency.\n",
    "2. Lower memory usage.\n",
    "3. Better accuracy.\n",
    "4. Parallel learning supported.\n",
    "5. Capable of handling large-scale data.\n",
    "\n",
    "Gradient Boosting Machines (or Gradient Boosted Trees, depending on who you ask to explain the acronym ‘GBM’) are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions and are considered among many Data Scientists to be the most versatile and useful predictive modeling algorithm. GBMs require very little preprocessing, elegantly handle missing data, strike a good balance between bias and variance and are typically able to find complicated interaction terms, which makes them a useful “swiss army knife” of predictive models. GBMs are a generalization of Freund and Schapire’s adaboost algorithm (1995) to handle arbitrary loss functions. They are very similar in concept to random forests, in that they fit individual decision trees to random re-samples of the input data, where each tree sees a boostrap sample of the rows of a the dataset and N arbitrarily chosen columns where N is a configural parameter of the model. GBMs differ from random forests in a single major aspect: rather than fitting the trees in parallel, the GBM fits each successive tree to the residual errors from all the previous trees combined. This is advantageous, as the model focuses each iteration on the examples that are most difficult to predict (and therefore most useful to get correct). Due to their iterative nature, GBMs are almost guaranteed to overfit the training data, given enough iterations. The 2 critial parameters of the algorithm; therefore, are the learning rate (or how fast the model fits the data) and the number of trees the model is allowed to fit. It is critical to cross-validate one of these 2 parameters.\n",
    "\n",
    "Early stopping is used to determine the best number of trees where overfitting begins. In this manner GBMs are usually capable of squeezing every last bit of information out of the training set and producing the model with the highest possible accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Detection Blender.png\n",
      "Auto-Tuned Word N-Gram Text Modeler using token occurrences.png\n",
      "DataRobotJupyter.png\n",
      "Jupyter.png\n",
      "LightGradientBoostingonElasticNetPredictions.png\n",
      "Nystroem Kernel SVM Classifier.png\n",
      "XGB.png\n",
      "XGB3.png\n",
      "XGB_2.png\n",
      "XGB_GammaLoss.png\n"
     ]
    }
   ],
   "source": [
    "!ls images/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/DataRobotJupyter.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='images/DataRobotJupyter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
