{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "western-nursery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Applications/anaconda3/lib/python3.7/site-packages (0.8.1rc1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfactory-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_FILE_URL = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n",
    "\n",
    "# Let's download the file and save it somewhere\n",
    "from requests import get\n",
    "with open('big.txt', 'wb') as big_f:\n",
    "    response = get(BIG_FILE_URL, )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        big_f.write(response.content)\n",
    "    else:\n",
    "        print(\"Unable to get the file: {}\".format(response.reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "packed-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
    "# the overall pipeline for various well-known tokenization algorithm. \n",
    "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFKC(),\n",
    "    Lowercase()\n",
    "])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "miniature-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
    "tokenizer.train(files=[\"big.txt\"], trainer=trainer)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "knowing-microphone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see the generated files in the output.\n",
    "tokenizer.model.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "material-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['Ġthis', 'Ġis', 'Ġa', 'Ġsimple', 'Ġin', 'put', 'Ġto', 'Ġbe', 'Ġtoken', 'ized']\n",
      "Decoded string:  this is a simple input to be tokenized\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "tokenizer.model = BPE('vocab.json', 'merges.txt')\n",
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-audio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-yeast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
